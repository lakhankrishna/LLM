# LLM


Please use the **Malicious_code_classification.ipynb** for downloading and run for the accurate Transformer pipelines are used for deployment of the model. Through pipeline integration with the model, they expedite the cleaning and tokenizing processes that occur before and after the model is processed. This allowed me for a variety of natural language processing (NLP) activities to be handled in parallel, which enhances efficiency. Pipelines are useful tools for real-world deployment since they allow monitoring performance and addressing any difficulties.


Reference :

Yin, X. and Ni, C., 2024. Multitask-based Evaluation of Open-Source LLM on Software Vulnerability. arXiv preprint arXiv:2404.02056.
Akuthota, V., Kasula, R., Sumona, S.T., Mohiuddin, M., Reza, M.T. and Rahman, M.M., 2023, November. Vulnerability Detection and Monitoring Using LLM. 

In 2023 IEEE 9th International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE) (pp. 309-314). IEEE.

Yesir, S. and Soğukpinar, İ., 2021, June. Malware detection and classification using fastText and BERT. In 2021 9th International Symposium on Digital Forensics and Security (ISDFS) (pp. 1-6). IEEE.

Yu, B., Tang, F., Ergu, D., Zeng, R., Ma, B. and Liu, F., 2024. Efficient Classification of Malicious URLs: M-BERT-A Modified BERT Variant for Enhanced Semantic Understanding. IEEE Access.

Rahali, A. and Akhloufi, M.A., 2021. MalBERT: Using transformers for cybersecurity and malicious software detection. arXiv preprint arXiv:2103.03806.

Souani, B., Khanfir, A., Bartel, A., Allix, K. and Le Traon, Y., 2022, June. Android malware detection using bert. In International Conference on Applied Cryptography and Network Security (pp. 575-591). Cham: Springer International Publishing.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. and Liu, P.J., 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140), pp.1-67.
